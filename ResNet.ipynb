{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba2a421",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Input, Conv2D, MaxPool2D, BatchNormalization, GlobalAveragePooling2D, AveragePooling2D, Flatten, ZeroPadding2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3507b079",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e380c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    def __init__(self, train_dir, buffer_size, batch_size, validation_split):\n",
    "        self.validation_split = 0.2\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.train_dir = pathlib.Path(train_dir)\n",
    "        self.CLASS_NAME = np.array([item.name for item in self.train_dir.glob('*') if item.name != 'LICENSE.txt'])\n",
    "        \n",
    "    def spliting_train_test(self):\n",
    "        full_dataset = tf.data.Dataset.list_files(str(self.train_dir/'*/*'))\n",
    "        full_dataset_size = len(full_dataset)\n",
    "        train_size = int((1-self.validation_split)*full_dataset_size)\n",
    "        train_dataset = full_dataset.take(train_size)\n",
    "        validation_dataset = full_dataset.skip(train_size)\n",
    "        return train_dataset, validation_dataset\n",
    "        #shayad ye returni bekhad\n",
    "    \n",
    "    def get_label(self, data):\n",
    "        p = tf.strings.split(data, os.sep)\n",
    "        p = p[-2] == self.CLASS_NAME\n",
    "        return tf.math.argmax(p, output_type=tf.int32)\n",
    "    \n",
    "    @staticmethod\n",
    "    def augmentation(image):\n",
    "        img = tf.image.resize_with_crop_or_pad(image, 180,180)\n",
    "        img = tf.image.random_crop(img, (150,150,3))\n",
    "        img = tf.image.random_brightness(img, max_delta=0.5)\n",
    "        return img\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_image(data):\n",
    "        img = tf.io.read_file(data)\n",
    "        img = tf.image.decode_image(img, 3, expand_animations=False)\n",
    "        img = tf.cast(img, tf.float32)\n",
    "        return img\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize(image):\n",
    "        return image/127. - 1\n",
    "    \n",
    "    @staticmethod\n",
    "    def resize(image,height, width):\n",
    "        image = tf.image.resize(image, (height, width),\n",
    "                                 method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "        #image = tf.image.resize_with_crop_or_pad(image, height, width)\n",
    "        return image\n",
    "    \n",
    "    def load_imge_with_label(self, data):\n",
    "        label = self.get_label(data)\n",
    "        img = self.load_image(data)\n",
    "        return img, label\n",
    "    \n",
    "    \n",
    "    def load_train_datset(self, data):\n",
    "        img, label = self.load_imge_with_label(data)\n",
    "        img = self.augmentation(img)\n",
    "        img = self.normalize(img)\n",
    "        return img, label\n",
    "    \n",
    "    def load_test_dataset(self, data):\n",
    "        img, label = self.load_imge_with_label(data)\n",
    "        img = self.resize(img, 150,150)\n",
    "        img = self.normalize(img)\n",
    "        return img, label\n",
    "    \n",
    "    def creating_dataset(self):\n",
    "        train_dataset, validation_dataset = self.spliting_train_test()\n",
    "        train_ds = train_dataset.map(self.load_train_datset)\n",
    "        test_ds = validation_dataset.map(self.load_test_dataset)\n",
    "        train_ds = train_ds.shuffle(1024).batch(64)\n",
    "        test_ds = test_ds.batch(64)\n",
    "        return train_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c65af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'F:\\\\TF\\\\tf.data\\\\flower_photos\\\\'\n",
    "data = Data(directory, 1024, 64, 0.2)\n",
    "train_dataset, validation_dataset = data.creating_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f6307e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentityBlock(Model):\n",
    "    def __init__(self, filters):\n",
    "        super(IdentityBlock, self).__init__()\n",
    "        f1, f2 = filters\n",
    "        self.conv1 = Conv2D(f1, kernel_size=(1, 1), strides=(1, 1), padding='valid', kernel_regularizer=l2(0.01))\n",
    "        self.bn1 = BatchNormalization()\n",
    "\n",
    "        self.conv2 =  Conv2D(f1, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(0.01))\n",
    "        self.bn2 = BatchNormalization()\n",
    "        \n",
    "        self.conv3 = Conv2D(f2, kernel_size=(1, 1), strides=(1, 1), padding='valid', kernel_regularizer=l2(0.01))\n",
    "        self.bn3 = BatchNormalization()\n",
    "\n",
    "        self.act = layers.Activation('relu')\n",
    "        self.add = layers.Add()\n",
    "\n",
    "    def call(self, inp):\n",
    "        x_skip = inp\n",
    "        x = self.conv1(inp)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.act(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.add([x_skip, x])\n",
    "        x = self.act(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeed8caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalBlock(Model):\n",
    "    def __init__(self, s, filters):\n",
    "        super(ConvolutionalBlock, self).__init__()\n",
    "        f1, f2 = filters\n",
    "        self.conv1 = Conv2D(f1, kernel_size=(1, 1), strides=(s, s), padding='valid', kernel_regularizer=l2(0.01))                         \n",
    "        self.bn1 = BatchNormalization()\n",
    "\n",
    "        self.conv2 = Conv2D(f1, kernel_size=(3, 3), strides=(1, 1), padding='same', kernel_regularizer=l2(0.01))\n",
    "        self.bn2 = BatchNormalization()\n",
    "        \n",
    "        self.conv3 = Conv2D(f2, kernel_size=(1, 1), strides=(1, 1), padding='valid', kernel_regularizer=l2(0.01))\n",
    "        self.bn3 = BatchNormalization()\n",
    "        \n",
    "        self.conv4 = Conv2D(f2, kernel_size=(1, 1), strides=(s, s), padding='valid', kernel_regularizer=l2(0.01))\n",
    "        self.bn4 = BatchNormalization()\n",
    "\n",
    "        self.act = layers.Activation('relu')\n",
    "        self.add = layers.Add()\n",
    "\n",
    "    def call(self, inp):\n",
    "        x_skip = inp\n",
    "        x = self.conv1(inp)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.act(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x_skip = self.conv4(x_skip)\n",
    "        x_skip = self.bn4(x_skip)\n",
    "        x = self.add([x_skip, x])\n",
    "        x = self.act(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e36efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(Model):\n",
    "    def __init__(self, num_class):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv = Conv2D(64, kernel_size=(7, 7), strides=(2, 2))\n",
    "        self.bn = BatchNormalization()\n",
    "        self.act = layers.Activation('relu')\n",
    "        self.max_pool = MaxPool2D((3, 3), strides=(2, 2))\n",
    "        \n",
    "        self.convb1 = ConvolutionalBlock(s=1, filters=(64, 256))\n",
    "        self.idyb1 = IdentityBlock(filters=(64, 256))\n",
    "        self.idyb2 = IdentityBlock(filters=(64, 256))\n",
    "        \n",
    "        self.convb2 = ConvolutionalBlock(s=2, filters=(128, 512))\n",
    "        self.idyb3 = IdentityBlock(filters=(128, 512))\n",
    "        self.idyb4 = IdentityBlock(filters=(128, 512))\n",
    "        self.idyb5 = IdentityBlock(filters=(128, 512))\n",
    "        \n",
    "        self.convb3 = ConvolutionalBlock(s=2, filters=(256, 1024))\n",
    "        self.idyb6 = IdentityBlock(filters=(256, 1024))\n",
    "        self.idyb7 = IdentityBlock(filters=(256, 1024))\n",
    "        self.idyb8 = IdentityBlock(filters=(256, 1024))\n",
    "        self.idyb9 = IdentityBlock(filters=(256, 1024))\n",
    "        self.idyb10 = IdentityBlock(filters=(256, 1024))\n",
    "        \n",
    "        self.convb4 = ConvolutionalBlock(s=2, filters=(512, 2048))\n",
    "        self.idyb11 = IdentityBlock(filters=(512, 2048))\n",
    "        self.idyb12 = IdentityBlock(filters=(512, 2048))\n",
    "        \n",
    "        self.ave_pool = AveragePooling2D((2, 2), padding='same')\n",
    "        self.classifier = Dense(num_class, activation='softmax', kernel_initializer='he_normal')\n",
    "\n",
    "    def call(self, inp):\n",
    "        x = ZeroPadding2D(padding=(3, 3))(inp)\n",
    "        x = self.conv(inp)\n",
    "        x = self.bn(x)\n",
    "        x = self.act(x)\n",
    "        x = self.max_pool(x)\n",
    "        \n",
    "        x = self.convb1(x)\n",
    "        x = self.idyb1(x)\n",
    "        x = self.idyb2(x)\n",
    "        \n",
    "        x = self.convb2(x)\n",
    "        x = self.idyb3(x)\n",
    "        x = self.idyb4(x)\n",
    "        x = self.idyb5(x)\n",
    "        \n",
    "        x = self.convb3(x)\n",
    "        x = self.idyb6(x)\n",
    "        x = self.idyb7(x)\n",
    "        x = self.idyb8(x)\n",
    "        x = self.idyb9(x)\n",
    "        x = self.idyb10(x)\n",
    "        \n",
    "        x = self.convb4(x)\n",
    "        x = self.idyb11(x)\n",
    "        x = self.idyb12(x)\n",
    "        \n",
    "        x = self.ave_pool(x)\n",
    "        x = Flatten()(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199ccb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = ResNet(5)\n",
    "resnet.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "history = resnet.fit(train_dataset, validation_data=validation_dataset, epochs=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
